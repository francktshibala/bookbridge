const { PrismaClient } = require('@prisma/client')
const prisma = new PrismaClient()

// Configuration for Romeo & Juliet
const BOOK_ID = 'gutenberg-1513' // Romeo & Juliet
const CEFR_LEVELS = ['A1', 'A2', 'B1', 'B2', 'C1', 'C2']
const BASE_URL = 'http://localhost:3005'  // Updated to match current server port
const BATCH_SIZE = 3 // Conservative for stability
const DELAY_BETWEEN_BATCHES = 8000 // 8 seconds between batches (avoid usage limit)
const MAX_RETRIES = 2
const DELAY_BETWEEN_REQUESTS = 2000 // 2s between individual requests (conservative)

async function sleep(ms) {
  return new Promise(resolve => setTimeout(resolve, ms))
}

async function getBookChunkCount() {
  try {
    // Get actual chunk count by testing API boundaries
    console.log('  üîç Testing API to find actual chunk count...')
    
    // Test each CEFR level to find minimum valid chunk count
    let minChunkCount = Infinity
    
    for (const level of CEFR_LEVELS.slice(0, 2)) { // Test first 2 levels only
      const response = await fetch(`${BASE_URL}/api/books/${BOOK_ID}/simplify?level=${level}&chunk=999&useAI=false`)
      
      if (!response.ok) {
        const data = await response.json()
        if (data.error && data.error.includes('has') && data.error.includes('chunks')) {
          const match = data.error.match(/has (\d+) chunks/)
          if (match) {
            const chunkCount = parseInt(match[1])
            console.log(`    ${level}: ${chunkCount} chunks available`)
            minChunkCount = Math.min(minChunkCount, chunkCount)
          }
        }
      }
    }
    
    if (minChunkCount === Infinity) {
      throw new Error('Could not determine chunk count from API')
    }
    
    console.log(`  ‚úÖ Detected chunk count: ${minChunkCount}`)
    return minChunkCount
  } catch (error) {
    console.error('Failed to get book chunk count:', error)
    throw error
  }
}

async function checkExistingSimplifications(totalChunks) {
  const existing = await prisma.bookSimplification.findMany({
    where: {
      bookId: BOOK_ID
    },
    select: {
      targetLevel: true,
      chunkIndex: true,
      qualityScore: true,
      originalText: true,
      simplifiedText: true
    }
  })
  
  // Check for identical text (failed simplifications)
  const identicalCount = existing.filter(item => 
    item.originalText === item.simplifiedText
  ).length
  
  const existingMap = new Set()
  existing.forEach(item => {
    existingMap.add(`${item.targetLevel}-${item.chunkIndex}`)
  })
  
  const missing = []
  for (const level of CEFR_LEVELS) {
    for (let chunk = 0; chunk < totalChunks; chunk++) {
      const key = `${level}-${chunk}`
      if (!existingMap.has(key)) {
        missing.push({ level, chunkIndex: chunk })
      }
    }
  }
  
  return { 
    existing: existing.length, 
    missing,
    identicalCount,
    validSimplifications: existing.length - identicalCount
  }
}

async function processSimplification(level, chunkIndex, retryCount = 0) {
  try {
    const url = `${BASE_URL}/api/books/${BOOK_ID}/simplify?level=${level}&chunk=${chunkIndex}&ai=true`
    
    console.log(`    ü§ñ Calling: ${url}`)
    
    const response = await fetch(url, {
      method: 'GET',
      headers: {
        'Content-Type': 'application/json'
      },
      timeout: 60000 // 60 second timeout for AI processing
    })
    
    if (!response.ok) {
      const errorText = await response.text()
      console.log(`    ‚ùå HTTP ${response.status}: ${errorText.substring(0, 200)}`)
      
      if (retryCount < MAX_RETRIES) {
        console.log(`    üîÑ Retrying in 3 seconds... (attempt ${retryCount + 2}/${MAX_RETRIES + 1})`)
        await sleep(3000)
        return processSimplification(level, chunkIndex, retryCount + 1)
      }
      return { success: false, level, chunkIndex, error: `HTTP ${response.status}` }
    }
    
    const result = await response.json()
    
    // Validate the result
    const isAIProcessed = result.source === 'ai_simplified' || 
                         (result.source === 'cache' && result.qualityScore < 1.0)
    const hasValidContent = result.content && result.content.length > 0
    const isDifferentFromOriginal = result.aiMetadata?.passedSimilarityGate !== false
    
    if (isAIProcessed && hasValidContent && isDifferentFromOriginal) {
      console.log(`    ‚úÖ Success: source=${result.source}, quality=${result.qualityScore?.toFixed(3)}, similarity=${result.aiMetadata?.similarity?.toFixed(3)}`)
      return { 
        success: true, 
        level, 
        chunkIndex, 
        quality: result.qualityScore,
        source: result.source,
        similarity: result.aiMetadata?.similarity
      }
    } else {
      console.log(`    ‚ö†Ô∏è  Warning: Poor quality - source=${result.source}, similarity=${result.aiMetadata?.similarity?.toFixed(3)}`)
      return { 
        success: false, 
        level, 
        chunkIndex, 
        error: `Poor quality: ${result.source}, similarity=${result.aiMetadata?.similarity?.toFixed(3)}` 
      }
    }
    
  } catch (error) {
    console.log(`    ‚ùå Error: ${error.message}`)
    
    if (retryCount < MAX_RETRIES) {
      console.log(`    üîÑ Retrying in 3 seconds... (attempt ${retryCount + 2}/${MAX_RETRIES + 1})`)
      await sleep(3000)
      return processSimplification(level, chunkIndex, retryCount + 1)
    }
    return { success: false, level, chunkIndex, error: error.message }
  }
}

async function processBatchSequentially(batch) {
  const results = []
  
  for (const item of batch) {
    console.log(`  üìù Processing: ${item.level} chunk ${item.chunkIndex}...`)
    const result = await processSimplification(item.level, item.chunkIndex)
    results.push(result)
    
    // Delay between requests to avoid rate limits
    if (results.length < batch.length) {
      console.log(`    ‚è±Ô∏è  Waiting ${DELAY_BETWEEN_REQUESTS}ms...`)
      await sleep(DELAY_BETWEEN_REQUESTS)
    }
  }
  
  const successful = results.filter(r => r.success).length
  const failed = results.filter(r => !r.success).length
  
  return { successful, failed, results }
}

async function main() {
  console.log('üé≠ BULK PROCESSING ROMEO & JULIET')
  console.log('='*50)
  console.log(`  Book ID: ${BOOK_ID}`)
  console.log(`  Server: ${BASE_URL}`)
  console.log(`  CEFR Levels: ${CEFR_LEVELS.join(', ')}`)
  
  try {
    // Step 1: Get total chunks
    console.log('\nüìä Analyzing book structure...')
    const totalChunks = await getBookChunkCount()
    console.log(`  Total chunks: ${totalChunks}`)
    console.log(`  Maximum possible simplifications: ${totalChunks * CEFR_LEVELS.length}`)
    
    // Step 2: Check existing simplifications
    console.log('\nüîç Checking existing simplifications...')
    const { existing, missing, identicalCount, validSimplifications } = await checkExistingSimplifications(totalChunks)
    console.log(`  Already processed: ${existing}`)
    console.log(`  Valid simplifications: ${validSimplifications}`)
    console.log(`  Identical text (failed): ${identicalCount}`)
    console.log(`  Missing: ${missing.length}`)
    
    if (identicalCount > 0) {
      console.log(`\n‚ö†Ô∏è  Warning: ${identicalCount} failed simplifications detected`)
      console.log(`   These show identical text and will be replaced during processing`)
    }
    
    if (missing.length === 0) {
      console.log('\n‚úÖ All simplifications already complete!')
      await prisma.$disconnect()
      return
    }
    
    // Group missing by level for better visibility
    const byLevel = {}
    CEFR_LEVELS.forEach(level => {
      byLevel[level] = missing.filter(m => m.level === level).length
    })
    console.log('\nüìä Missing by level:')
    Object.entries(byLevel).forEach(([level, count]) => {
      console.log(`  ${level}: ${count} chunks`)
    })
    
    // Step 3: Process missing simplifications
    console.log('\nüìù Processing missing simplifications...')
    console.log(`  Batch size: ${BATCH_SIZE}`)
    console.log(`  Delay between batches: ${DELAY_BETWEEN_BATCHES}ms`)
    console.log(`  Delay between requests: ${DELAY_BETWEEN_REQUESTS}ms`)
    console.log(`  Estimated time: ${Math.ceil(missing.length * (15 + DELAY_BETWEEN_REQUESTS/1000) / 60)} minutes`)
    
    let totalSuccessful = 0
    let totalFailed = 0
    const failedItems = []
    
    for (let i = 0; i < missing.length; i += BATCH_SIZE) {
      const batch = missing.slice(i, i + BATCH_SIZE)
      const batchNum = Math.floor(i / BATCH_SIZE) + 1
      const totalBatches = Math.ceil(missing.length / BATCH_SIZE)
      
      console.log(`\nüì¶ Batch ${batchNum}/${totalBatches} (${batch.length} items):`)
      
      const { successful, failed, results } = await processBatchSequentially(batch)
      totalSuccessful += successful
      totalFailed += failed
      
      // Track failed items for retry
      results.filter(r => !r.success).forEach(r => failedItems.push(r))
      
      console.log(`  Batch results: ${successful} successful, ${failed} failed`)
      
      // Progress update
      const progress = ((i + batch.length) / missing.length * 100).toFixed(1)
      console.log(`  Overall progress: ${progress}% (${totalSuccessful}/${missing.length} successful)`)
      
      // Stop if too many failures
      if (failed === batch.length && batch.length > 1) {
        console.log('\n‚ö†Ô∏è  All items in batch failed. Stopping to prevent further errors.')
        console.log('  Please check server status and usage limits.')
        break
      }
      
      // Delay between batches
      if (i + BATCH_SIZE < missing.length) {
        console.log(`  ‚è±Ô∏è  Waiting ${DELAY_BETWEEN_BATCHES}ms before next batch...`)
        await sleep(DELAY_BETWEEN_BATCHES)
      }
    }
    
    // Step 4: Final summary
    console.log('\n' + '='*50)
    console.log('üìä FINAL SUMMARY:')
    console.log(`  Total processed: ${totalSuccessful + totalFailed}`)
    console.log(`  Successful: ${totalSuccessful}`)
    console.log(`  Failed: ${totalFailed}`)
    
    if (failedItems.length > 0) {
      console.log('\n‚ùå Failed items (for manual retry):')
      failedItems.slice(0, 10).forEach(item => {
        console.log(`  ${item.level} chunk ${item.chunkIndex}: ${item.error}`)
      })
      if (failedItems.length > 10) {
        console.log(`  ... and ${failedItems.length - 10} more`)
      }
    }
    
    // Verify completion
    const { existing: finalExisting, missing: finalMissing } = await checkExistingSimplifications(totalChunks)
    console.log(`\n‚úÖ Final status:`)
    console.log(`  Total simplifications in database: ${finalExisting}`)
    console.log(`  Remaining missing: ${finalMissing.length}`)
    
    if (finalMissing.length === 0) {
      console.log('\nüéâ ROMEO & JULIET FULLY PROCESSED!')
      console.log(`üé≠ Shakespeare modernized for all CEFR levels!`)
    } else {
      console.log('\n‚è≥ Processing incomplete. Run script again to continue.')
    }
    
  } catch (error) {
    console.error('\n‚ùå Fatal error:', error)
  } finally {
    await prisma.$disconnect()
  }
}

// Check if server is running before starting
console.log(`üîç Checking server at ${BASE_URL}...`)
fetch(`${BASE_URL}/api/health`)
  .catch(() => {
    // Try to detect if server is running by calling a simple API
    return fetch(`${BASE_URL}/api/books/${BOOK_ID}/content-fast`)
  })
  .then(() => {
    console.log('‚úÖ Server is running')
    main()
  })
  .catch(() => {
    console.error(`‚ùå Server is not running at ${BASE_URL}`)
    console.error('   Please ensure the dev server is running with: npm run dev')
    console.error('   If server is on different port, update BASE_URL in script')
    process.exit(1)
  })